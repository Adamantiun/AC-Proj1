{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airlines Delay\n",
    "\n",
    "### Project developed by:\n",
    "- Adam Nogueira (up202007519)\n",
    "- Eduardo Silva (up202004999)\n",
    "- João Félix (up202008867)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This project involves developing a data mining case study, which is described in a separate document provided on Moodle. The main focus of the project is a predictive data mining task, the details of which are outlined in the case study description.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bibliography\n",
    "NumPy Developers, Numpy documentation, URL: https://numpy.org/doc/stable/user/index.html#user <br>\n",
    "pandas development team, pandas documentation, URL: https://pandas.pydata.org/docs/user_guide/index.html#user-guide<br>\n",
    "Matplotlib Development team, Matplotlib documentation, URL: https://matplotlib.org/stable/index.html <br>\n",
    "scikit-learn developers, scikit-learn documentation, URL: https://scikit-learn.org/0.18/documentation.html<br>\n",
    "\n",
    "### Approach\n",
    "\n",
    "The approach to this project was done as follows:\n",
    "\n",
    "1. **Data analysis**: First we analyzed the dataset to inspect for the need for data pre-processing: checked the corresponding histograms, class distribution, and the existence of missing or null values.\n",
    "2. **Algorithm implementation**: Flowing that, we defined the training and test sets using train/test split, resampled the dataset, and applied the SciKit Learn's algorithms to obtain the first results.\n",
    "3. **Evaluation and refinement**: After analyzing the first results, tunning of each algorithm was done utilizing the SciKit Learn GridSearchCV to find the parameters of each algorithm that yielded the best overall results, and evaluated the final results.\n",
    "\n",
    "### Used Libraries\n",
    "\n",
    "- **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "- **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "- **matplotlib**: The essential Machine Learning package in Python.\n",
    "- **sklearn**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "- **seaborn**: Advanced statistical plotting library.\n",
    "- **pycaret**: Offers streamlined workflows and a wide range of pre-built algorithms and techniques to experiment with different models and compare their performance using different evaluation metrics.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "We start by importing the required libraries and plotting some graphs for initial analysis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:17:43.243651300Z",
     "start_time": "2023-10-13T11:17:39.240954700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\duart\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "     year  lgID  tmID  franchID  confID  divID  rank  playoff  seeded  \\\n",
      "0       9   NaN   NaN       NaN     NaN    NaN     7      NaN       0   \n",
      "1      10   NaN   NaN       NaN     NaN    NaN     2      NaN       0   \n",
      "2       1   NaN   NaN       NaN     NaN    NaN     8      NaN       0   \n",
      "3       2   NaN   NaN       NaN     NaN    NaN     4      NaN       0   \n",
      "4       3   NaN   NaN       NaN     NaN    NaN     2      NaN       0   \n",
      "..    ...   ...   ...       ...     ...    ...   ...      ...     ...   \n",
      "137     6   NaN   NaN       NaN     NaN    NaN     5      NaN       0   \n",
      "138     7   NaN   NaN       NaN     NaN    NaN     4      NaN       0   \n",
      "139     8   NaN   NaN       NaN     NaN    NaN     5      NaN       0   \n",
      "140     9   NaN   NaN       NaN     NaN    NaN     6      NaN       0   \n",
      "141    10   NaN   NaN       NaN     NaN    NaN     4      NaN       0   \n",
      "\n",
      "     firstRound  ...  opptmORB  opptmDRB  opptmTRB  won  lost  GP  homeW  \\\n",
      "0           NaN  ...         0         0         0    4    30  34      1   \n",
      "1           NaN  ...         0         0         0   18    16  34     12   \n",
      "2           NaN  ...         0         0         0    8    24  32      5   \n",
      "3           NaN  ...         0         0         0   18    14  32     11   \n",
      "4           NaN  ...         0         0         0   18    14  32     11   \n",
      "..          ...  ...       ...       ...       ...  ...   ...  ..    ...   \n",
      "137         NaN  ...         0         0         0   16    18  34     10   \n",
      "138         NaN  ...         0         0         0   18    16  34     13   \n",
      "139         NaN  ...         0         0         0   16    18  34      8   \n",
      "140         NaN  ...         0         0         0   10    24  34      6   \n",
      "141         NaN  ...         0         0         0   16    18  34     11   \n",
      "\n",
      "     homeL  awayW  awayL  \n",
      "0       16      3     14  \n",
      "1        5      6     11  \n",
      "2       11      3     13  \n",
      "3        5      7      9  \n",
      "4        5      7      9  \n",
      "..     ...    ...    ...  \n",
      "137      7      6     11  \n",
      "138      4      5     12  \n",
      "139      9      8      9  \n",
      "140     11      4     13  \n",
      "141      6      5     12  \n",
      "\n",
      "[142 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import data_manip\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from enum import Enum\n",
    "import seaborn as sb\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pycaret.classification import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "\n",
    "# Set the warning filter to \"ignore\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "awards = pd.read_csv('modified_data/awards_players.csv', na_values=['NA'], delimiter=\",\")\n",
    "coaches = pd.read_csv('modified_data/coaches.csv', na_values=['NA'], delimiter=\",\")\n",
    "players = pd.read_csv('modified_data/players.csv', na_values=['NA'], delimiter=\",\")\n",
    "players_teams = pd.read_csv('modified_data/players_teams.csv', na_values=['NA'], delimiter=\",\")\n",
    "series_post = pd.read_csv('modified_data/series_post.csv', na_values=['NA'], delimiter=\",\")\n",
    "teams = pd.read_csv('modified_data/teams.csv', na_values=['NA'], delimiter=\",\")\n",
    "teams_post = pd.read_csv('modified_data/teams_post.csv', na_values=['NA'], delimiter=\",\")\n",
    "\n",
    "\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.600120800Z",
     "start_time": "2023-10-13T11:13:58.379867800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             year  lgID  tmID  franchID  confID  divID        rank  playoff  \\\ncount  142.000000   0.0   0.0       0.0     0.0    0.0  142.000000      0.0   \nmean     5.302817   NaN   NaN       NaN     NaN    NaN    4.084507      NaN   \nstd      2.917274   NaN   NaN       NaN     NaN    NaN    2.095226      NaN   \nmin      1.000000   NaN   NaN       NaN     NaN    NaN    1.000000      NaN   \n25%      3.000000   NaN   NaN       NaN     NaN    NaN    2.000000      NaN   \n50%      5.000000   NaN   NaN       NaN     NaN    NaN    4.000000      NaN   \n75%      8.000000   NaN   NaN       NaN     NaN    NaN    6.000000      NaN   \nmax     10.000000   NaN   NaN       NaN     NaN    NaN    8.000000      NaN   \n\n       seeded  firstRound  ...  opptmORB  opptmDRB  opptmTRB         won  \\\ncount   142.0         0.0  ...     142.0     142.0     142.0  142.000000   \nmean      0.0         NaN  ...       0.0       0.0       0.0   16.661972   \nstd       0.0         NaN  ...       0.0       0.0       0.0    4.999131   \nmin       0.0         NaN  ...       0.0       0.0       0.0    4.000000   \n25%       0.0         NaN  ...       0.0       0.0       0.0   13.000000   \n50%       0.0         NaN  ...       0.0       0.0       0.0   17.000000   \n75%       0.0         NaN  ...       0.0       0.0       0.0   20.000000   \nmax       0.0         NaN  ...       0.0       0.0       0.0   28.000000   \n\n             lost          GP       homeW       homeL       awayW       awayL  \ncount  142.000000  142.000000  142.000000  142.000000  142.000000  142.000000  \nmean    16.661972   33.323944   10.169014    6.492958    6.492958   10.169014  \nstd      4.999131    0.949425    2.994017    2.967308    2.702104    2.731409  \nmin      4.000000   32.000000    1.000000    0.000000    1.000000    3.000000  \n25%     14.000000   32.000000    8.000000    4.250000    5.000000    9.000000  \n50%     16.000000   34.000000   11.000000    6.000000    6.000000   10.000000  \n75%     20.000000   34.000000   12.000000    8.000000    8.000000   12.000000  \nmax     30.000000   34.000000   16.000000   16.000000   13.000000   16.000000  \n\n[8 rows x 56 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>lgID</th>\n      <th>tmID</th>\n      <th>franchID</th>\n      <th>confID</th>\n      <th>divID</th>\n      <th>rank</th>\n      <th>playoff</th>\n      <th>seeded</th>\n      <th>firstRound</th>\n      <th>...</th>\n      <th>opptmORB</th>\n      <th>opptmDRB</th>\n      <th>opptmTRB</th>\n      <th>won</th>\n      <th>lost</th>\n      <th>GP</th>\n      <th>homeW</th>\n      <th>homeL</th>\n      <th>awayW</th>\n      <th>awayL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>142.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>142.000000</td>\n      <td>0.0</td>\n      <td>142.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>142.0</td>\n      <td>142.0</td>\n      <td>142.0</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.302817</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.084507</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16.661972</td>\n      <td>16.661972</td>\n      <td>33.323944</td>\n      <td>10.169014</td>\n      <td>6.492958</td>\n      <td>6.492958</td>\n      <td>10.169014</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.917274</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.095226</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.999131</td>\n      <td>4.999131</td>\n      <td>0.949425</td>\n      <td>2.994017</td>\n      <td>2.967308</td>\n      <td>2.702104</td>\n      <td>2.731409</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>32.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.000000</td>\n      <td>14.000000</td>\n      <td>32.000000</td>\n      <td>8.000000</td>\n      <td>4.250000</td>\n      <td>5.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.000000</td>\n      <td>16.000000</td>\n      <td>34.000000</td>\n      <td>11.000000</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>34.000000</td>\n      <td>12.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>28.000000</td>\n      <td>30.000000</td>\n      <td>34.000000</td>\n      <td>16.000000</td>\n      <td>16.000000</td>\n      <td>13.000000</td>\n      <td>16.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 56 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom formatting function\n",
    "def custom_format(value):\n",
    "    # Check if the value is a number (int or float)\n",
    "    if isinstance(value, (int, float)):\n",
    "        # If it's an integer, format as an integer\n",
    "        if isinstance(value, int):\n",
    "            return value\n",
    "        # If it's a float, format with 2 decimal places\n",
    "        elif isinstance(value, float):\n",
    "            return round(value, 2)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "formatted_df = teams.applymap(custom_format)\n",
    "formatted_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.600120800Z",
     "start_time": "2023-10-13T11:13:58.509918400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "bioID         0\npos          78\nbirthDate     0\ndtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.600120800Z",
     "start_time": "2023-10-13T11:13:58.534633700Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AirportFrom'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'AirportFrom'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m data_encoded \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Combine \"AirportFrom\" and \"AirportTo\" columns to create a unified set of airports\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m airports \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mdata_encoded\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAirportFrom\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m, data_encoded[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAirportTo\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Perform label encoding on the combined set of airports\u001B[39;00m\n\u001B[0;32m      7\u001B[0m label_encoder_airports \u001B[38;5;241m=\u001B[39m LabelEncoder()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'AirportFrom'"
     ]
    }
   ],
   "source": [
    "data_encoded = dataset.copy()\n",
    "\n",
    "# Combine \"AirportFrom\" and \"AirportTo\" columns to create a unified set of airports\n",
    "airports = pd.concat([data_encoded['AirportFrom'], data_encoded['AirportTo']])\n",
    "\n",
    "# Perform label encoding on the combined set of airports\n",
    "label_encoder_airports = LabelEncoder()\n",
    "encoded_airports = label_encoder_airports.fit_transform(airports)\n",
    "\n",
    "# Perform label encoding on the \"Airline\" column\n",
    "label_encoder_airline = LabelEncoder()\n",
    "data_encoded['Airline'] = label_encoder_airline.fit_transform(data_encoded['Airline'])\n",
    "\n",
    "# Update \"AirportFrom\" and \"AirportTo\" columns with the encoded values\n",
    "data_encoded['AirportFrom'] = encoded_airports[:len(data_encoded)]\n",
    "data_encoded['AirportTo'] = encoded_airports[len(data_encoded):]\n",
    "\n",
    "# Remove the \"Flight\" column\n",
    "data_encoded = data_encoded.drop('Flight', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the correlation of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.673637500Z",
     "start_time": "2023-10-13T11:13:58.608526200Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_corr = data_encoded.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "mask = np.zeros_like(dataset_corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "sb.heatmap(dataset_corr, cmap=\"YlGnBu\", annot=True, square=True, mask=mask, fmt='.2f', annot_kws={\"size\": 10});\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.608526200Z"
    }
   },
   "outputs": [],
   "source": [
    "data_encoded.hist(bins=30, figsize=(30, 16), sharey=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relation between the day of the week and delay in flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.608526200Z"
    }
   },
   "outputs": [],
   "source": [
    "sb.catplot(x=\"DayOfWeek\", kind=\"count\", data=data_encoded, hue=\"Class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relation between the airline and delay in flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.615769200Z"
    }
   },
   "outputs": [],
   "source": [
    "sb.catplot(x=\"Airline\", kind=\"count\", data=data_encoded, hue=\"Class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is slightly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.619556400Z"
    }
   },
   "outputs": [],
   "source": [
    "fraud_df_pie_chart = data_encoded.loc[data_encoded['Class'] == 1.0]\n",
    "not_fraud_df_pie_chart = data_encoded.loc[data_encoded['Class'] == 0.0]\n",
    "\n",
    "array_pie_chart = np.array([len(fraud_df_pie_chart), len(not_fraud_df_pie_chart)])\n",
    "pie_chart_labels = [\"Delayed\", \"Not Delayed\"]\n",
    "\n",
    "plt.pie(array_pie_chart, labels=pie_chart_labels, autopct='%.2f')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset and assessing its characteristics, we conducted a comprehensive analysis. The results revealed a high level of data consistency, with no missing values or notable outliers observed. As a consequence, the dataset demonstrated a remarkable level of readiness for analysis, requiring minimal data preprocessing efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.621598700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_encoded.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos os dados em conjunto de input e label para os classificadores do Scikit. Label é a coluna Class and input é as restantes colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.624924700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_encoded['Class'] = data_encoded['Class'].astype('category')\n",
    "\n",
    "col_names = list(data_encoded.columns)\n",
    "col_names.remove('Class')\n",
    "\n",
    "inputs = data_encoded[col_names].values\n",
    "labels = data_encoded['Class'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumidamente dividi dados em dados de teste e treinamento, mantendo a mesma distribuição das classes inicias, usando 1/4 do dataset original\n",
    "\n",
    "- stratify - para manter a distribuição de classes \n",
    "- train_in - variável que armazena as características de treinamento\n",
    "- test_in - variável que armazena as características de teste\n",
    "- train_classes - armazena as classes dos dados de treinamento\n",
    "- test_classes - armazena as classes dos dados de testes\n",
    "- random_state - para garantir randomness na divisão de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.628505Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(train_in,\n",
    " test_in,\n",
    " train_classes,\n",
    " test_classes) = train_test_split(inputs, labels, test_size=0.25, random_state=1, stratify=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analyses showed us that our working dataset is unbalanced. We implemented both undersampling and oversampling. Undersampling removes samples from majority categories, while oversampling duplicates samples from minority categories. Generally oversampling is preferred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conta a ocorrência de cada classe nos conjuntos de treinamento e teste. No training set, tem 224338 de 0 e 180198 de 1. Da um overview da distribuição das classes em cada conunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.628505Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"---Train Set---\")\n",
    "print(Counter(train_classes))\n",
    "print(\"\\n---Test Set---\")\n",
    "print(Counter(test_classes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É usado para balancear os dados. Remove aleatoriamente dados da classe com maior número de dados até que se encontro o equilíbrio que se quer. Undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.632829Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "us_inputs, us_labels = rus.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(us_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz oversampling dos dados e conseguimos ver que realmente os dados ficaram equilibradoos nos dados de treino. aumenta o número de linhas, tornando-a mais proporcionsl à classe maioritaria<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.632829Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ros = SMOTE()\n",
    "\n",
    "os_inputs, os_labels = ros.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(os_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a StandardScaler from SciKit Learn's preprocessing library to standardize the data. Porque é necessário para o K nearest neighbrs e o SVM\n",
    "\n",
    "Fas-se a padronização dos dados para garantir que as características contribuam igualmente para os modelos de machine learning, e evita que uma caraterística em particular dominee o processo de learning da outra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.638382Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_in)\n",
    "train_in = scaler.fit_transform(train_in)\n",
    "test_in = scaler.fit_transform(test_in)\n",
    "\n",
    "scaler.fit(os_inputs)\n",
    "os_inputs = scaler.fit_transform(os_inputs)\n",
    "\n",
    "scaler.fit(us_inputs)\n",
    "us_inputs = scaler.fit_transform(us_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.640483400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(train_in, train_classes)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "knn_og_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.640483400Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(us_inputs, us_labels)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "knn_confusion_matrix = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(knn_confusion_matrix)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print(result1)\n",
    "\n",
    "knn_us_report = classification_report(test_classes, y_pred, output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.647495200Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(os_inputs, os_labels)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "\n",
    "knn_os_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:\n",
    "TP FP\n",
    "TN FN\n",
    "FP = False Positive - deu que iam atrasar (1) mas na realidade é 0\n",
    "\n",
    "Precision - mede a proporção de dados corretamente calculado TP/(TP+FP)\n",
    "\n",
    "Accuracy - mede a correção no geral (TP + TN) / (TP + TN + FP + FN).\n",
    "    - em dados não balenciados pode ser misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.648933300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(train_in, train_classes)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Original dataset ---\\n\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(f\"{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\")\n",
    "print(f\"{classification_report(test_classes, dtc_prediction)}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.648933300Z"
    }
   },
   "outputs": [],
   "source": [
    "dtc.fit(us_inputs, us_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_us_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Undersampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.658240300Z"
    }
   },
   "outputs": [],
   "source": [
    "dtc.fit(os_inputs, os_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_os_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Oversampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.659552700Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "svc.fit(train_in, train_classes)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_og_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.659552700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(us_inputs, us_labels)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_us_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.665088400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(os_inputs, os_labels)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_os_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.665088400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "all_algorithms_data = pd.read_csv('models_comparison.csv', na_values=['NA'], delimiter=\",\")\n",
    "all_algorithms_data.set_index(\"Model\", inplace=True)\n",
    "\n",
    "sb.heatmap(all_algorithms_data, cmap=\"YlGnBu\", annot=True)\n",
    "plt.xlabel('Results')\n",
    "plt.ylabel('ML Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_algorithms_data = pd.read_csv('models_comparison_selected.csv', na_values=['NA'], delimiter=\",\")\n",
    "selected_algorithms_data.set_index(\"Model\", inplace=True)\n",
    "\n",
    "sb.heatmap(selected_algorithms_data, cmap=\"YlGnBu\", annot=True)\n",
    "plt.xlabel('Results')\n",
    "plt.ylabel('ML Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Accuracys of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Accuracy'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Accuracy', data=selected_algorithms_data, color='#A7226E')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Accuracy Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing AUC (Area Under Curve) of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['AUC'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='AUC', data=selected_algorithms_data, color='#FE4365')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('AUC Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Recall of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Recall'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Recall', data=selected_algorithms_data, color='#9DE0AD')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Recall Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Precision of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.893868Z",
     "start_time": "2023-10-13T11:13:58.673637500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Precision'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Precision', data=selected_algorithms_data, color='#F7DB4F')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Precision Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing F1-score of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.673637500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['F1'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='F1', data=selected_algorithms_data, color='#F26B38')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('F1-Score Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1-Score')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Kappa of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.675723400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Kappa'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Kappa', data=selected_algorithms_data, color='#2F9599')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Kappa Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Kappa')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Matthews Correlation Coefficient of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.677950400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['MCC'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='MCC', data=selected_algorithms_data, color='#FF4E50')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Matthews Correlation Coefficient Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Matthews Correlation Coefficient')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Training Time (sec) of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.678506400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['TT (Sec)'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='TT (Sec)', data=selected_algorithms_data, color='#9DE0AD')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Training Time comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (Sec)')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
