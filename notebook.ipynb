{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project - WNBA Playoffs prediction\n",
    "\n",
    "### Project developed by:\n",
    "- Adam Nogueira (up202007519)\n",
    "- Eduardo Silva (up202004999)\n",
    "- João Félix (up202008867)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "### Introduction\n",
    "This project involves developing a data mining case study, which is described in a separate document provided on Moodle. The main focus of the project is a predictive data mining task, the details of which are outlined in the case study description.\n",
    "\n",
    "### Bibliography\n",
    "NumPy Developers, Numpy documentation, URL: https://numpy.org/doc/stable/user/index.html#user <br>\n",
    "pandas development team, pandas documentation, URL: https://pandas.pydata.org/docs/user_guide/index.html#user-guide<br>\n",
    "Matplotlib Development team, Matplotlib documentation, URL: https://matplotlib.org/stable/index.html <br>\n",
    "scikit-learn developers, scikit-learn documentation, URL: https://scikit-learn.org/0.18/documentation.html<br>\n",
    "\n",
    "### Approach\n",
    "\n",
    "The approach to this project was done as follows:\n",
    "\n",
    "1. **Data analysis**: First we analyzed the dataset to inspect for the need for data pre-processing: checked the corresponding histograms, class distribution, and the existence of missing or null values.\n",
    "2. **Algorithm implementation**: Flowing that, we defined the training and test sets using train/test split, resampled the dataset, and applied the SciKit Learn's algorithms to obtain the first results.\n",
    "3. **Evaluation and refinement**: After analyzing the first results, tunning of each algorithm was done utilizing the SciKit Learn GridSearchCV to find the parameters of each algorithm that yielded the best overall results, and evaluated the final results.\n",
    "\n",
    "### Used Libraries\n",
    "\n",
    "- **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "- **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "- **matplotlib**: The essential Machine Learning package in Python.\n",
    "- **sklearn**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "- **seaborn**: Advanced statistical plotting library.\n",
    "- **pycaret**: Offers streamlined workflows and a wide range of pre-built algorithms and techniques to experiment with different models and compare their performance using different evaluation metrics.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "We start by importing the required libraries and plotting some graphs for initial analysis of the dataset.\n",
    "\n",
    "### Key Statistics\n",
    "\n",
    "- Win-Loss Record: This is the most straightforward indicator. Teams with more wins are more likely to make the Playoff prediction predictions. Historically, teams with around a .500 or better win-loss record tend to have a good chance of making the playoffs.\n",
    "\n",
    "- Winning Percentage: Similar to win-loss record, winning percentage (Wins / Total Games) is a fundamental metric used to assess a team's performance.\n",
    "\n",
    "- Points Per Game (PPG): Teams that score more points on average are often more successful. This statistic reflects a team's offensive efficiency.\n",
    "\n",
    "- Points Allowed Per Game (PAPG): Teams that allow fewer points per game have a stronger defense. Defensive efficiency is a critical factor in determining playoff success.\n",
    "\n",
    "- Net Rating: Net rating is the difference between a team's offensive rating (points scored per 100 possessions) and their defensive rating (points allowed per 100 possessions). Teams with positive net ratings are often playoff-bound.\n",
    "\n",
    "- Field Goal Percentage (FG%): Shooting efficiency is a key factor in a team's offensive performance. A high field goal percentage indicates effective shooting.\n",
    "\n",
    "- Three-Point Percentage (3P%): The ability to make three-point shots is crucial in modern basketball. Teams with high three-point percentages often perform well.\n",
    "\n",
    "- Free Throw Percentage (FT%): Teams with good free throw shooting can close out close games more effectively.\n",
    "\n",
    "- Rebounds Per Game (RPG): Rebounding is a key component of both offense and defense. Teams that dominate the boards tend to have an advantage.\n",
    "\n",
    "- Assists Per Game (APG): Ball movement and sharing are critical in the NBA. Teams with high assist numbers often have a strong offense.\n",
    "\n",
    "- Steals and Blocks: Defensive statistics such as steals and blocks indicate a team's ability to disrupt the opponent's offense and protect the rim.\n",
    "\n",
    "- Turnovers: Reducing turnovers is important for maintaining possession and minimizing scoring opportunities for the opposition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:17:43.243651300Z",
     "start_time": "2023-10-13T11:17:39.240954700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imbalanced-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "     year  lgID tmID franchID confID  rank playoff firstRound semis finals  \\\n",
      "0       9  WNBA  ATL      ATL     EA     7       N        NaN   NaN    NaN   \n",
      "1      10  WNBA  ATL      ATL     EA     2       Y          L   NaN    NaN   \n",
      "2       1  WNBA  CHA      CHA     EA     8       N        NaN   NaN    NaN   \n",
      "3       2  WNBA  CHA      CHA     EA     4       Y          W     W      L   \n",
      "4       3  WNBA  CHA      CHA     EA     2       Y          L   NaN    NaN   \n",
      "..    ...   ...  ...      ...    ...   ...     ...        ...   ...    ...   \n",
      "137     6  WNBA  WAS      WAS     EA     5       N        NaN   NaN    NaN   \n",
      "138     7  WNBA  WAS      WAS     EA     4       Y          L   NaN    NaN   \n",
      "139     8  WNBA  WAS      WAS     EA     5       N        NaN   NaN    NaN   \n",
      "140     9  WNBA  WAS      WAS     EA     6       N        NaN   NaN    NaN   \n",
      "141    10  WNBA  WAS      WAS     EA     4       Y          L   NaN    NaN   \n",
      "\n",
      "     ... d_to  d_blk  d_pts  won  lost  GP  homeW  homeL  awayW  awayL  \n",
      "0    ...  561    134   2879    4    30  34      1     16      3     14  \n",
      "1    ...  601    133   2797   18    16  34     12      5      6     11  \n",
      "2    ...  426    123   2429    8    24  32      5     11      3     13  \n",
      "3    ...  447    124   2009   18    14  32     11      5      7      9  \n",
      "4    ...  424    103   2133   18    14  32     11      5      7      9  \n",
      "..   ...  ...    ...    ...  ...   ...  ..    ...    ...    ...    ...  \n",
      "137  ...  504    108   2305   16    18  34     10      7      6     11  \n",
      "138  ...  573     89   2657   18    16  34     13      4      5     12  \n",
      "139  ...  579    103   2639   16    18  34      8      9      8      9  \n",
      "140  ...  530    146   2601   10    24  34      6     11      4     13  \n",
      "141  ...  576    156   2622   16    18  34     11      6      5     12  \n",
      "\n",
      "[142 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "import data_manip\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from enum import Enum\n",
    "import seaborn as sb\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pycaret.classification import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "\n",
    "# Set the warning filter to \"ignore\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "awards = pd.read_csv('modified_data/awards_players.csv', na_values=['NA'], delimiter=\",\")\n",
    "coaches = pd.read_csv('modified_data/coaches.csv', na_values=['NA'], delimiter=\",\")\n",
    "players = pd.read_csv('modified_data/players.csv', na_values=['NA'], delimiter=\",\")\n",
    "players_teams = pd.read_csv('modified_data/players_teams.csv', na_values=['NA'], delimiter=\",\")\n",
    "series_post = pd.read_csv('modified_data/series_post.csv', na_values=['NA'], delimiter=\",\")\n",
    "teams = pd.read_csv('modified_data/teams.csv', na_values=['NA'], delimiter=\",\")\n",
    "teams_post = pd.read_csv('modified_data/teams_post.csv', na_values=['NA'], delimiter=\",\")\n",
    "\n",
    "\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns fgMade/fgAttempted, ftMade/ftAttempted, threeMade/threeAttempted converted to ratio columns and new file saved as 'modified_data/players_teams.csv'\n",
      "\n",
      "Columns fgMade, fgAttempted, ftMade, ftAttempted, threeMade, threeAttempted removed and new file saved as 'modified_data/players_teams.csv'\n",
      "\n",
      "Columns confW, confL, min, attend, arena, tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB, divID, seeded removed and new file saved as 'modified_data/teams.csv'\n",
      "\n",
      "Columns firstseason, lastseason, height, weight, college, collegeOther, deathDate removed and new file saved as 'modified_data/players.csv'\n",
      "\n",
      "Columns renamed and new file saved as 'modified_data/players.csv'\n",
      "\n",
      "Columns award, lgID removed and new file saved as 'modified_data/awards_players.csv'\n",
      "\n",
      "Columns lgID removed and new file saved as 'modified_data/teams_post.csv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PLAYERS_TEAMS\n",
    "player_teams_input_path = 'original_data/players_teams.csv'\n",
    "player_teams_column_pairs = [('fgMade', 'fgAttempted'), ('ftMade', 'ftAttempted'), ('threeMade', 'threeAttempted')]\n",
    "player_teams_output_path = 'modified_data/players_teams.csv'\n",
    "convert_columns_to_ratio(player_teams_input_path, player_teams_column_pairs, player_teams_output_path)\n",
    "\n",
    "player_teams_columns_to_exclude = ['fgMade', 'fgAttempted', 'ftMade', 'ftAttempted', 'threeMade', 'threeAttempted']\n",
    "exclude_columns(player_teams_output_path, player_teams_columns_to_exclude, player_teams_output_path)\n",
    "\n",
    "# TEAMS\n",
    "teams_input_path = 'original_data/teams.csv'\n",
    "teams_columns_to_exclude = ['confW', 'confL', 'min', 'attend', 'arena', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB', 'divID', 'seeded']\n",
    "teams_output_path = 'modified_data/teams.csv'\n",
    "\n",
    "exclude_columns(teams_input_path, teams_columns_to_exclude, teams_output_path)\n",
    "\n",
    "# PLAYERS\n",
    "players_input_path = 'original_data/players.csv'\n",
    "players_columns_to_exclude = ['firstseason', 'lastseason', 'height', 'weight', 'college', 'collegeOther', 'deathDate']\n",
    "players_output_path = 'modified_data/players.csv'\n",
    "exclude_columns(players_input_path, players_columns_to_exclude, players_output_path)\n",
    "\n",
    "column_mapping = {'bioID': 'playerID'}\n",
    "rename_columns(players_output_path, column_mapping, players_output_path)\n",
    "\n",
    "\n",
    "# AWARDS_PLAYERS\n",
    "players_input_path = 'original_data/awards_players.csv'\n",
    "players_columns_to_exclude = ['award', 'lgID']\n",
    "players_output_path = 'modified_data/awards_players.csv'\n",
    "exclude_columns(players_input_path, players_columns_to_exclude, players_output_path)\n",
    "\n",
    "# TEAMS_POST\n",
    "teams_input_path = 'original_data/teams_post.csv'\n",
    "teams_columns_to_exclude = ['lgID']\n",
    "teams_output_path = 'modified_data/teams_post.csv'\n",
    "\n",
    "exclude_columns(teams_input_path, teams_columns_to_exclude, teams_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('modified_data/teams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.600120800Z",
     "start_time": "2023-10-13T11:13:58.379867800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>rank</th>\n",
       "      <th>o_fgm</th>\n",
       "      <th>o_fga</th>\n",
       "      <th>o_ftm</th>\n",
       "      <th>o_fta</th>\n",
       "      <th>o_3pm</th>\n",
       "      <th>o_3pa</th>\n",
       "      <th>o_oreb</th>\n",
       "      <th>o_dreb</th>\n",
       "      <th>...</th>\n",
       "      <th>d_to</th>\n",
       "      <th>d_blk</th>\n",
       "      <th>d_pts</th>\n",
       "      <th>won</th>\n",
       "      <th>lost</th>\n",
       "      <th>GP</th>\n",
       "      <th>homeW</th>\n",
       "      <th>homeL</th>\n",
       "      <th>awayW</th>\n",
       "      <th>awayL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.302817</td>\n",
       "      <td>4.084507</td>\n",
       "      <td>860.387324</td>\n",
       "      <td>2039.683099</td>\n",
       "      <td>488.338028</td>\n",
       "      <td>651.366197</td>\n",
       "      <td>157.161972</td>\n",
       "      <td>463.014085</td>\n",
       "      <td>330.500000</td>\n",
       "      <td>730.929577</td>\n",
       "      <td>...</td>\n",
       "      <td>510.450704</td>\n",
       "      <td>122.070423</td>\n",
       "      <td>2366.260563</td>\n",
       "      <td>16.661972</td>\n",
       "      <td>16.661972</td>\n",
       "      <td>33.323944</td>\n",
       "      <td>10.169014</td>\n",
       "      <td>6.492958</td>\n",
       "      <td>6.492958</td>\n",
       "      <td>10.169014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.917274</td>\n",
       "      <td>2.095226</td>\n",
       "      <td>86.998969</td>\n",
       "      <td>176.879707</td>\n",
       "      <td>70.749372</td>\n",
       "      <td>86.035246</td>\n",
       "      <td>43.736580</td>\n",
       "      <td>116.166119</td>\n",
       "      <td>41.191432</td>\n",
       "      <td>83.378114</td>\n",
       "      <td>...</td>\n",
       "      <td>54.038019</td>\n",
       "      <td>20.658537</td>\n",
       "      <td>234.615384</td>\n",
       "      <td>4.999131</td>\n",
       "      <td>4.999131</td>\n",
       "      <td>0.949425</td>\n",
       "      <td>2.994017</td>\n",
       "      <td>2.967308</td>\n",
       "      <td>2.702104</td>\n",
       "      <td>2.731409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1740.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>1788.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>794.500000</td>\n",
       "      <td>1908.500000</td>\n",
       "      <td>435.250000</td>\n",
       "      <td>582.750000</td>\n",
       "      <td>128.250000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>301.250000</td>\n",
       "      <td>653.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>470.250000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>2196.750000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>864.000000</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>483.500000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>333.500000</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>2339.500000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>915.000000</td>\n",
       "      <td>2177.500000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>716.500000</td>\n",
       "      <td>180.750000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>356.750000</td>\n",
       "      <td>788.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>545.500000</td>\n",
       "      <td>136.750000</td>\n",
       "      <td>2522.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>2485.000000</td>\n",
       "      <td>668.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>802.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>931.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>3031.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             year        rank        o_fgm        o_fga       o_ftm  \\\n",
       "count  142.000000  142.000000   142.000000   142.000000  142.000000   \n",
       "mean     5.302817    4.084507   860.387324  2039.683099  488.338028   \n",
       "std      2.917274    2.095226    86.998969   176.879707   70.749372   \n",
       "min      1.000000    1.000000   647.000000  1740.000000  333.000000   \n",
       "25%      3.000000    2.000000   794.500000  1908.500000  435.250000   \n",
       "50%      5.000000    4.000000   864.000000  2025.000000  483.500000   \n",
       "75%      8.000000    6.000000   915.000000  2177.500000  539.000000   \n",
       "max     10.000000    8.000000  1128.000000  2485.000000  668.000000   \n",
       "\n",
       "            o_fta       o_3pm       o_3pa      o_oreb      o_dreb  ...  \\\n",
       "count  142.000000  142.000000  142.000000  142.000000  142.000000  ...   \n",
       "mean   651.366197  157.161972  463.014085  330.500000  730.929577  ...   \n",
       "std     86.035246   43.736580  116.166119   41.191432   83.378114  ...   \n",
       "min    469.000000   62.000000  205.000000  242.000000  537.000000  ...   \n",
       "25%    582.750000  128.250000  389.000000  301.250000  653.250000  ...   \n",
       "50%    650.000000  157.000000  459.000000  333.500000  724.000000  ...   \n",
       "75%    716.500000  180.750000  528.000000  356.750000  788.000000  ...   \n",
       "max    882.000000  283.000000  802.000000  452.000000  931.000000  ...   \n",
       "\n",
       "             d_to       d_blk        d_pts         won        lost  \\\n",
       "count  142.000000  142.000000   142.000000  142.000000  142.000000   \n",
       "mean   510.450704  122.070423  2366.260563   16.661972   16.661972   \n",
       "std     54.038019   20.658537   234.615384    4.999131    4.999131   \n",
       "min    390.000000   71.000000  1788.000000    4.000000    4.000000   \n",
       "25%    470.250000  109.000000  2196.750000   13.000000   14.000000   \n",
       "50%    503.000000  123.000000  2339.500000   17.000000   16.000000   \n",
       "75%    545.500000  136.750000  2522.750000   20.000000   20.000000   \n",
       "max    649.000000  206.000000  3031.000000   28.000000   30.000000   \n",
       "\n",
       "               GP       homeW       homeL       awayW       awayL  \n",
       "count  142.000000  142.000000  142.000000  142.000000  142.000000  \n",
       "mean    33.323944   10.169014    6.492958    6.492958   10.169014  \n",
       "std      0.949425    2.994017    2.967308    2.702104    2.731409  \n",
       "min     32.000000    1.000000    0.000000    1.000000    3.000000  \n",
       "25%     32.000000    8.000000    4.250000    5.000000    9.000000  \n",
       "50%     34.000000   11.000000    6.000000    6.000000   10.000000  \n",
       "75%     34.000000   12.000000    8.000000    8.000000   12.000000  \n",
       "max     34.000000   16.000000   16.000000   13.000000   16.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom formatting function\n",
    "def custom_format(value):\n",
    "    # Check if the value is a number (int or float)\n",
    "    if isinstance(value, (int, float)):\n",
    "        # If it's an integer, format as an integer\n",
    "        if isinstance(value, int):\n",
    "            return value\n",
    "        # If it's a float, format with 2 decimal places\n",
    "        elif isinstance(value, float):\n",
    "            return round(value, 2)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "formatted_df = teams.applymap(custom_format)\n",
    "formatted_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.600120800Z",
     "start_time": "2023-10-13T11:13:58.509918400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year            0\n",
       "lgID            0\n",
       "tmID            0\n",
       "franchID        0\n",
       "confID          0\n",
       "rank            0\n",
       "playoff         0\n",
       "firstRound     62\n",
       "semis         104\n",
       "finals        122\n",
       "name            0\n",
       "o_fgm           0\n",
       "o_fga           0\n",
       "o_ftm           0\n",
       "o_fta           0\n",
       "o_3pm           0\n",
       "o_3pa           0\n",
       "o_oreb          0\n",
       "o_dreb          0\n",
       "o_reb           0\n",
       "o_asts          0\n",
       "o_pf            0\n",
       "o_stl           0\n",
       "o_to            0\n",
       "o_blk           0\n",
       "o_pts           0\n",
       "d_fgm           0\n",
       "d_fga           0\n",
       "d_ftm           0\n",
       "d_fta           0\n",
       "d_3pm           0\n",
       "d_3pa           0\n",
       "d_oreb          0\n",
       "d_dreb          0\n",
       "d_reb           0\n",
       "d_asts          0\n",
       "d_pf            0\n",
       "d_stl           0\n",
       "d_to            0\n",
       "d_blk           0\n",
       "d_pts           0\n",
       "won             0\n",
       "lost            0\n",
       "GP              0\n",
       "homeW           0\n",
       "homeL           0\n",
       "awayW           0\n",
       "awayL           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset and assessing its characteristics, we conducted a comprehensive analysis. The results revealed a high level of data consistency, with no missing values or notable outliers observed. As a consequence, the dataset demonstrated a remarkable level of readiness for analysis, requiring minimal data preprocessing efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.621598700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142 entries, 0 to 141\n",
      "Data columns (total 48 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   year        142 non-null    int64 \n",
      " 1   lgID        142 non-null    object\n",
      " 2   tmID        142 non-null    object\n",
      " 3   franchID    142 non-null    object\n",
      " 4   confID      142 non-null    object\n",
      " 5   rank        142 non-null    int64 \n",
      " 6   playoff     142 non-null    object\n",
      " 7   firstRound  80 non-null     object\n",
      " 8   semis       38 non-null     object\n",
      " 9   finals      20 non-null     object\n",
      " 10  name        142 non-null    object\n",
      " 11  o_fgm       142 non-null    int64 \n",
      " 12  o_fga       142 non-null    int64 \n",
      " 13  o_ftm       142 non-null    int64 \n",
      " 14  o_fta       142 non-null    int64 \n",
      " 15  o_3pm       142 non-null    int64 \n",
      " 16  o_3pa       142 non-null    int64 \n",
      " 17  o_oreb      142 non-null    int64 \n",
      " 18  o_dreb      142 non-null    int64 \n",
      " 19  o_reb       142 non-null    int64 \n",
      " 20  o_asts      142 non-null    int64 \n",
      " 21  o_pf        142 non-null    int64 \n",
      " 22  o_stl       142 non-null    int64 \n",
      " 23  o_to        142 non-null    int64 \n",
      " 24  o_blk       142 non-null    int64 \n",
      " 25  o_pts       142 non-null    int64 \n",
      " 26  d_fgm       142 non-null    int64 \n",
      " 27  d_fga       142 non-null    int64 \n",
      " 28  d_ftm       142 non-null    int64 \n",
      " 29  d_fta       142 non-null    int64 \n",
      " 30  d_3pm       142 non-null    int64 \n",
      " 31  d_3pa       142 non-null    int64 \n",
      " 32  d_oreb      142 non-null    int64 \n",
      " 33  d_dreb      142 non-null    int64 \n",
      " 34  d_reb       142 non-null    int64 \n",
      " 35  d_asts      142 non-null    int64 \n",
      " 36  d_pf        142 non-null    int64 \n",
      " 37  d_stl       142 non-null    int64 \n",
      " 38  d_to        142 non-null    int64 \n",
      " 39  d_blk       142 non-null    int64 \n",
      " 40  d_pts       142 non-null    int64 \n",
      " 41  won         142 non-null    int64 \n",
      " 42  lost        142 non-null    int64 \n",
      " 43  GP          142 non-null    int64 \n",
      " 44  homeW       142 non-null    int64 \n",
      " 45  homeL       142 non-null    int64 \n",
      " 46  awayW       142 non-null    int64 \n",
      " 47  awayL       142 non-null    int64 \n",
      "dtypes: int64(39), object(9)\n",
      "memory usage: 53.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos os dados em conjunto de input e label para os classificadores do Scikit. Label é a coluna Class and input é as restantes colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.624924700Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['playoff'] = dataset['playoff'].astype('category')\n",
    "\n",
    "col_names = list(dataset.columns)\n",
    "col_names.remove('name')\n",
    "col_names.remove('lgID')\n",
    "col_names.remove('tmID')\n",
    "col_names.remove('franchID')\n",
    "col_names.remove('confID')\n",
    "col_names.remove('firstRound')\n",
    "col_names.remove('semis')\n",
    "col_names.remove('finals')\n",
    "\n",
    "inputs = dataset[col_names].values\n",
    "labels = dataset['playoff'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumidamente dividi dados em dados de teste e treinamento, mantendo a mesma distribuição das classes inicias, usando 1/4 do dataset original\n",
    "\n",
    "- stratify - para manter a distribuição de classes \n",
    "- train_in - variável que armazena as características de treinamento\n",
    "- test_in - variável que armazena as características de teste\n",
    "- train_classes - armazena as classes dos dados de treinamento\n",
    "- test_classes - armazena as classes dos dados de testes\n",
    "- random_state - para garantir randomness na divisão de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.628505Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(train_in,\n",
    " test_in,\n",
    " train_classes,\n",
    " test_classes) = train_test_split(inputs, labels, test_size=0.25, random_state=1, stratify=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analyses showed us that our working dataset is unbalanced. We implemented both undersampling and oversampling. Undersampling removes samples from majority categories, while oversampling duplicates samples from minority categories. Generally oversampling is preferred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conta a ocorrência de cada classe nos conjuntos de treinamento e teste. Da um overview da distribuição das classes em cada conunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.628505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Train Set---\n",
      "Counter({'Y': 60, 'N': 46})\n",
      "\n",
      "---Test Set---\n",
      "Counter({'Y': 20, 'N': 16})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"---Train Set---\")\n",
    "print(Counter(train_classes))\n",
    "print(\"\\n---Test Set---\")\n",
    "print(Counter(test_classes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É usado para balancear os dados. Remove aleatoriamente dados da classe com maior número de dados até que se encontro o equilíbrio que se quer. Undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.632829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'N': 46, 'Y': 46})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "us_inputs, us_labels = rus.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(us_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz oversampling dos dados e conseguimos ver que realmente os dados ficaram equilibradoos nos dados de treino. aumenta o número de linhas, tornando-a mais proporcionsl à classe maioritaria<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.632829Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\AC\\notebook.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/AC/notebook.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/AC/notebook.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ros \u001b[39m=\u001b[39m SMOTE()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/AC/notebook.ipynb#Y112sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m os_inputs, os_labels \u001b[39m=\u001b[39m ros\u001b[39m.\u001b[39;49mfit_resample(train_in, train_classes)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/AC/notebook.ipynb#Y112sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(Counter(os_labels))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:82\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m check_classification_targets(y)\n\u001b[0;32m     81\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 82\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:156\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    154\u001b[0m     accept_sparse \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    155\u001b[0m y, binarize_y \u001b[39m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 156\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse)\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'N'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ros = SMOTE()\n",
    "\n",
    "os_inputs, os_labels = ros.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(os_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a StandardScaler from SciKit Learn's preprocessing library to standardize the data. Porque é necessário para o K nearest neighbrs e o SVM\n",
    "\n",
    "Fas-se a padronização dos dados para garantir que as características contribuam igualmente para os modelos de machine learning, e evita que uma caraterística em particular dominee o processo de learning da outra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.638382Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_in)\n",
    "train_in = scaler.fit_transform(train_in)\n",
    "test_in = scaler.fit_transform(test_in)\n",
    "\n",
    "scaler.fit(os_inputs)\n",
    "os_inputs = scaler.fit_transform(os_inputs)\n",
    "\n",
    "scaler.fit(us_inputs)\n",
    "us_inputs = scaler.fit_transform(us_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.640483400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(train_in, train_classes)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "knn_og_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.640483400Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(us_inputs, us_labels)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "knn_confusion_matrix = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(knn_confusion_matrix)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print(result1)\n",
    "\n",
    "knn_us_report = classification_report(test_classes, y_pred, output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.647495200Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(os_inputs, os_labels)\n",
    "y_pred = classifier.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "\n",
    "knn_os_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:\n",
    "TP FP\n",
    "TN FN\n",
    "FP = False Positive - deu que iam atrasar (1) mas na realidade é 0\n",
    "\n",
    "Precision - mede a proporção de dados corretamente calculado TP/(TP+FP)\n",
    "\n",
    "Accuracy - mede a correção no geral (TP + TN) / (TP + TN + FP + FN).\n",
    "    - em dados não balenciados pode ser misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.648933300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(train_in, train_classes)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Original dataset ---\\n\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(f\"{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\")\n",
    "print(f\"{classification_report(test_classes, dtc_prediction)}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.648933300Z"
    }
   },
   "outputs": [],
   "source": [
    "dtc.fit(us_inputs, us_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_us_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Undersampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.658240300Z"
    }
   },
   "outputs": [],
   "source": [
    "dtc.fit(os_inputs, os_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_os_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Oversampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.659552700Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "svc.fit(train_in, train_classes)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_og_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.659552700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(us_inputs, us_labels)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_us_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.665088400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(os_inputs, os_labels)\n",
    "y_pred = svc.predict(test_in)\n",
    "\n",
    "result = confusion_matrix(test_classes, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(test_classes, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1) \n",
    "\n",
    "svc_os_report = classification_report(test_classes, y_pred,output_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.665088400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "all_algorithms_data = pd.read_csv('models_comparison.csv', na_values=['NA'], delimiter=\",\")\n",
    "all_algorithms_data.set_index(\"Model\", inplace=True)\n",
    "\n",
    "sb.heatmap(all_algorithms_data, cmap=\"YlGnBu\", annot=True)\n",
    "plt.xlabel('Results')\n",
    "plt.ylabel('ML Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_algorithms_data = pd.read_csv('models_comparison_selected.csv', na_values=['NA'], delimiter=\",\")\n",
    "selected_algorithms_data.set_index(\"Model\", inplace=True)\n",
    "\n",
    "sb.heatmap(selected_algorithms_data, cmap=\"YlGnBu\", annot=True)\n",
    "plt.xlabel('Results')\n",
    "plt.ylabel('ML Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Accuracys of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Accuracy'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Accuracy', data=selected_algorithms_data, color='#A7226E')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Accuracy Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing AUC (Area Under Curve) of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['AUC'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='AUC', data=selected_algorithms_data, color='#FE4365')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('AUC Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Recall of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.668635500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Recall'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Recall', data=selected_algorithms_data, color='#9DE0AD')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Recall Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Precision of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:13:58.893868Z",
     "start_time": "2023-10-13T11:13:58.673637500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Precision'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Precision', data=selected_algorithms_data, color='#F7DB4F')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Precision Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing F1-score of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.673637500Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['F1'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='F1', data=selected_algorithms_data, color='#F26B38')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('F1-Score Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1-Score')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Kappa of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.675723400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['Kappa'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='Kappa', data=selected_algorithms_data, color='#2F9599')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Kappa Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Kappa')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Matthews Correlation Coefficient of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.677950400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['MCC'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='MCC', data=selected_algorithms_data, color='#FF4E50')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Matthews Correlation Coefficient Comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Matthews Correlation Coefficient')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Training Time (sec) of each algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T11:13:58.678506400Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_algorithms_data = selected_algorithms_data.sort_values(by=['TT (Sec)'], ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.barplot(x=selected_algorithms_data.index, y='TT (Sec)', data=selected_algorithms_data, color='#9DE0AD')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Training Time comparison by Algorithm')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (Sec)')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
